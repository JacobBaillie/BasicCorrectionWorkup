{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717bbe2-38d7-4653-b0ef-ef50a037468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folder? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_45656\\3056305553.py:62: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCD file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_45656\\3056305553.py:62: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===== Paths to input folders and output folder =====\n",
    "data_folder = r\"Z:\\Jacob\\Material CrPS4\\Undoped\\09102025 Linear Polarization with 45deg block\\CCD Start\\Polarization_aprox40K\"\n",
    "correction_file_path = r\"Z:\\Jacob\\Correction Files\\Downstairs PL table March 2024\\G2_correction.csv\"  # Path to the correction CSV file\\\n",
    "#correction_file_path = r\"Z:\\Jacob\\Correction Files\\Lamp_corr_halogen_old.csv\"  # Path to the correction CSV file\n",
    "fibercorrection_file_path = r\"Z:\\Jacob\\Correction Files\\Fiber for MCPL\\Fiber correction.csv\"  # Path to the Fiber correction CSV file\n",
    "baseline_file_path = r\"Z:\\Jacob\\Material CrPS4\\Undoped\\09102025 Linear Polarization with 45deg block\\dark_HeNe_650SP_808LP_fiber_200umG2_5s_975nm.arc_data\"   # Path to the Baseline arcdata file\n",
    "output_folder = os.path.join(data_folder, \"output\")\n",
    "data_type = \"CCD\" # CCD or PMT\n",
    "PMT_skiprows = 10\n",
    "PL_type = \"PL\" #PL or PLE for jacobian correction (PLE doesn't need it)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ===== Integration Range (cm-1) =====\n",
    "# Set integration_min and integration_max to None for full range\n",
    "integration_min = 10000   # e.g., 12000\n",
    "integration_max = 999999   # e.g., 14000\n",
    "\n",
    "# ===== Spike removal =====\n",
    "spike_threshold = 1000  # e.g., 5e5. Set to None to disable.\n",
    "\n",
    "# ===== Baseline subtraction mode =====\n",
    "# 0 = interpolate from baseline file\n",
    "# 1 = subtract average from baseline file\n",
    "# 2 = subtract user-specified constant\n",
    "# 3 = subtract average of lowest N values in each data file\n",
    "# 4 = interpolate from baseline file then subtract subtract average of lowest N values in each data file\n",
    "baseline_mode = 0  # Change this value to choose mode\n",
    "user_defined_baseline =  2260  # Used only if baseline_mode == 2\n",
    "lowest_n_values = 20         # Used only if baseline_mode == 3\n",
    "\n",
    "# ===== Enable or disable fiber correction =====\n",
    "use_fiber_correction = True  # Set to False to skip applying the fiber correction\n",
    "\n",
    "# ===== Enable or disable normalization =====\n",
    "normalization_option = False\n",
    "\n",
    "# ===== Store integration results =====\n",
    "integrated_signals = []\n",
    "spectrum_ids = []\n",
    "spectrum_times = []  # store timestamps for ordering\n",
    "\n",
    "# Clear existing files in the output folder\n",
    "for file in os.listdir(output_folder):\n",
    "    file_path = os.path.join(output_folder, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n",
    "print(\"Found folder?\", os.path.exists(data_folder))\n",
    "\n",
    "# Helper function to read data after the blank line\n",
    "def read_CCD_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Find the first blank line\n",
    "    blank_line_index = next(i for i, line in enumerate(lines) if line.strip() == \"\")\n",
    "    # Read the data starting after the blank line\n",
    "    data = pd.read_csv(\n",
    "        filepath,\n",
    "        delim_whitespace=True,\n",
    "        skiprows=blank_line_index + 1,  # Skip lines up to and including the blank line\n",
    "        header=None,\n",
    "        names=[\"Wavelength\", \"PL\"]\n",
    "    )\n",
    "    # Ensure columns are numeric\n",
    "    data[\"Wavelength\"] = pd.to_numeric(data[\"Wavelength\"], errors=\"coerce\")\n",
    "    data[\"PL\"] = pd.to_numeric(data[\"PL\"], errors=\"coerce\")\n",
    "    # Drop rows with NaN values\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def read_PMT_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Read the data\n",
    "    data = pd.read_csv(\n",
    "        filepath,\n",
    "        sep='\\t',\n",
    "        skiprows=PMT_skiprows,  # 6 header + 1 column name line\n",
    "        header=None,\n",
    "        names=[\"Wavelength\", \"PL\"]\n",
    "    )\n",
    "    # Ensure columns are numeric\n",
    "    data[\"Wavelength\"] = pd.to_numeric(data[\"Wavelength\"], errors=\"coerce\")\n",
    "    data[\"PL\"] = pd.to_numeric(data[\"PL\"], errors=\"coerce\")\n",
    "    # Drop rows with NaN values\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def despike_spectrum(y, threshold):\n",
    "    \"\"\"\n",
    "    Replace any values above threshold with the average of neighbors.\n",
    "    Handles consecutive spikes by iterating until all fixed.\n",
    "    \"\"\"\n",
    "    if threshold is None:\n",
    "        return y\n",
    "\n",
    "    y = y.copy()\n",
    "    n = len(y)\n",
    "    changed = True\n",
    "\n",
    "    while changed:  # repeat in case there are consecutive spikes\n",
    "        changed = False\n",
    "        for i in range(1, n - 1):  # skip edges\n",
    "            if y[i] > threshold:\n",
    "                # Replace with neighbor average\n",
    "                y[i] = 0.5 * (y[i - 1] + y[i + 1])\n",
    "                changed = True\n",
    "\n",
    "    return y\n",
    "\n",
    "# Load baseline file\n",
    "baseline_data = read_CCD_file(baseline_file_path)\n",
    "\n",
    "# Load correction file from CSV (skip the first row)\n",
    "correction_data = pd.read_csv(correction_file_path, skiprows=1, header=None, names=[\"Wavelength\", \"PL\"])\n",
    "\n",
    "# Load fiber correction file from CSV (skip the first row)\n",
    "fiber_correction_data = pd.read_csv(fibercorrection_file_path, skiprows=1, header=None, names=[\"Wavelength\", \"PL\"])\n",
    "\n",
    "# Ensure columns are numeric\n",
    "correction_data[\"Wavelength\"] = pd.to_numeric(correction_data[\"Wavelength\"], errors=\"coerce\")\n",
    "correction_data[\"PL\"] = pd.to_numeric(correction_data[\"PL\"], errors=\"coerce\")\n",
    "# Drop rows with NaN values\n",
    "correction_data.dropna(inplace=True)\n",
    "\n",
    "# Load data file\n",
    "#data = read_CCD_file(data_file)\n",
    "\n",
    "first_data_filename = None\n",
    "first_raw_data = None\n",
    "first_corrected_data = None\n",
    "# Process each data file in the folder (excluding the baseline file)\n",
    "for filename in os.listdir(data_folder):\n",
    "    input_path = os.path.join(data_folder, filename)\n",
    "    if os.path.isfile(input_path):\n",
    "        if data_type == \"CCD\":\n",
    "            print (\"CCD file\")\n",
    "            data = read_CCD_file(input_path)\n",
    "        if data_type == \"PMT\":\n",
    "            print (\"PMT file\")\n",
    "            data = read_PMT_file(input_path)\n",
    "            \n",
    "        if first_data_filename is None:\n",
    "            first_data_filename = filename\n",
    "            first_raw_data = data.copy()\n",
    "    \n",
    "        # Interpolate corrections to match data wavelengths\n",
    "        correction_interp = np.interp(\n",
    "            data[\"Wavelength\"].values.astype(float),\n",
    "            correction_data[\"Wavelength\"].values.astype(float),\n",
    "            correction_data[\"PL\"].values.astype(float)\n",
    "        )\n",
    "        if use_fiber_correction:\n",
    "            fiber_correction_interp = np.interp(\n",
    "                data[\"Wavelength\"].values.astype(float),\n",
    "                fiber_correction_data[\"Wavelength\"].values.astype(float),\n",
    "                fiber_correction_data[\"PL\"].values.astype(float)\n",
    "            )\n",
    "        else:\n",
    "            fiber_correction_interp = np.ones_like(data[\"Wavelength\"].values)\n",
    "    \n",
    "    \n",
    "        # Subtract baseline using selected mode\n",
    "        if baseline_mode == 0:\n",
    "            # Interpolated baseline subtraction\n",
    "            interpolated_baseline = np.interp(\n",
    "                data[\"Wavelength\"].values.astype(float),\n",
    "                baseline_data[\"Wavelength\"].values.astype(float),\n",
    "                baseline_data[\"PL\"].values.astype(float)\n",
    "            )\n",
    "            data[\"PL\"] -= interpolated_baseline\n",
    "    \n",
    "        elif baseline_mode == 1:\n",
    "            # Average baseline PL value subtraction\n",
    "            avg_baseline_PL = baseline_data[\"PL\"].mean()\n",
    "            data[\"PL\"] -= avg_baseline_PL\n",
    "    \n",
    "        elif baseline_mode == 2:\n",
    "            # Subtract user-defined constant\n",
    "            data[\"PL\"] -= user_defined_baseline\n",
    "    \n",
    "        elif baseline_mode == 3:\n",
    "            # Subtract average of lowest N values from the current data\n",
    "            sorted_pl = np.sort(data[\"PL\"].values)\n",
    "            avg_lowest_n = np.mean(sorted_pl[:lowest_n_values])\n",
    "            data[\"PL\"] -= avg_lowest_n\n",
    "\n",
    "        elif baseline_mode == 4:\n",
    "            # Interpolate from baseline file then subtract subtract average of lowest N values in each data file\n",
    "            interpolated_baseline = np.interp(\n",
    "                data[\"Wavelength\"].values.astype(float),\n",
    "                baseline_data[\"Wavelength\"].values.astype(float),\n",
    "                baseline_data[\"PL\"].values.astype(float)\n",
    "            )\n",
    "            data[\"PL\"] -= interpolated_baseline\n",
    "            sorted_pl = np.sort(data[\"PL\"].values)\n",
    "            avg_lowest_n = np.mean(sorted_pl[:lowest_n_values])\n",
    "            data[\"PL\"] -= avg_lowest_n\n",
    "\n",
    "        # Despike if enabled\n",
    "        if spike_threshold is not None:\n",
    "            data[\"PL\"] = despike_spectrum(data[\"PL\"].values, spike_threshold)\n",
    "            \n",
    "        # Apply corrections\n",
    "        data[\"PL\"] *= correction_interp\n",
    "        data[\"PL\"] *= fiber_correction_interp\n",
    "    \n",
    "        # Convert to X_cm and update PL scale\n",
    "        data[\"X_cm\"] = 10000000 / data[\"Wavelength\"]\n",
    "        if PL_type == \"PLE\":\n",
    "            data[\"PL\"] = (data[\"PL\"] * data[\"Wavelength\"] ** 2) / 10000000\n",
    "    \n",
    "        #Normalize\n",
    "        if normalization_option:\n",
    "            norm_factor = sum(data[\"PL\"])\n",
    "            data[\"PL\"]/=norm_factor\n",
    "            \n",
    "        # Save corrected data\n",
    "        output_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_corr_cm.arc_data\")\n",
    "        data[[\"X_cm\", \"PL\"]].to_csv(output_file, sep=\"\\t\", index=False, header=False)\n",
    "        print(f\"Saved: {output_file}\")\n",
    "\n",
    "       # Integrate the corrected PL over the spectrum\n",
    "        if integration_min is not None and integration_max is not None:\n",
    "            mask = (data[\"X_cm\"] >= integration_min) & (data[\"X_cm\"] <= integration_max)\n",
    "            x_vals = data.loc[mask, \"X_cm\"].values\n",
    "            y_vals = data.loc[mask, \"PL\"].values\n",
    "        else:\n",
    "            x_vals = data[\"X_cm\"].values\n",
    "            y_vals = data[\"PL\"].values\n",
    "\n",
    "        # Integrate\n",
    "        integrated_area = np.trapz(y_vals, x_vals)\n",
    "\n",
    "        # Flip sign if x-axis is descending\n",
    "        if x_vals[0] > x_vals[-1]:\n",
    "            integrated_area *= -1\n",
    "        integrated_signals.append(integrated_area)\n",
    "        spectrum_ids.append(os.path.splitext(filename)[0])\n",
    "        file_time = os.path.getmtime(input_path)  # seconds since epoch\n",
    "        spectrum_times.append(file_time)\n",
    "    \n",
    "        if filename == first_data_filename:\n",
    "            first_corrected_data = data.copy()\n",
    "\n",
    "\n",
    "# --- PLOT 1: Integrated PL vs Spectrum ID ---\n",
    "# --- PLOT 3: Integrated PL vs Spectrum ID ---\n",
    "if integrated_signals:\n",
    "    # Sort by time of collection\n",
    "    results = pd.DataFrame({\n",
    "        \"Spectrum_ID\": spectrum_ids,\n",
    "        \"Integrated_PL\": integrated_signals,\n",
    "        \"Timestamp\": spectrum_times\n",
    "    })\n",
    "    results.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    integration_output_file = os.path.join(output_folder, \"integrated_PL_vs_ID.csv\")\n",
    "    results[[\"Spectrum_ID\", \"Integrated_PL\"]].to_csv(integration_output_file, index=False)\n",
    "    print(f\"Saved integration results: {integration_output_file}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results[\"Spectrum_ID\"], results[\"Integrated_PL\"], marker='o', linestyle='-', color='purple')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Spectrum ID\")\n",
    "    plt.ylabel(\"Integrated PL (a.u.)\")\n",
    "    if integration_min is not None and integration_max is not None:\n",
    "        plt.title(f\"Integrated PL vs Spectrum ID ({integration_min}-{integration_max} cm⁻¹)\")\n",
    "    else:\n",
    "        plt.title(\"Integrated PL vs Spectrum ID (Full Range)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- PLOT 2: Final corrected output data ---\n",
    "if first_corrected_data is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(first_corrected_data[\"X_cm\"], first_corrected_data[\"PL\"], label=\"Corrected PL\", color='red')\n",
    "\n",
    "    plt.xlabel(\"Wavenumber (1/cm)\")\n",
    "    plt.ylabel(\"Corrected PL Intensity (a.u.)\")\n",
    "    plt.title(f\"Corrected Output: {first_data_filename}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# --- PLOT 3: Raw data with correction and fiber correction files ---\n",
    "if first_raw_data is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(first_raw_data[\"Wavelength\"], first_raw_data[\"PL\"], label=\"Raw PL\", color='gray')\n",
    "    plt.plot(correction_data[\"Wavelength\"], correction_data[\"PL\"], label=\"Correction File\", linestyle=\"--\", color='blue')\n",
    "    plt.plot(fiber_correction_data[\"Wavelength\"], fiber_correction_data[\"PL\"], label=\"Fiber Correction File\", linestyle=\"--\", color='green')\n",
    "    #plt.xlim(800, 950)\n",
    "    plt.xlabel(\"Wavelength (nm)\")\n",
    "    plt.ylabel(\"PL Intensity (a.u.)\")\n",
    "    plt.title(f\"Raw + Corrections: {first_data_filename}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaece961-3a33-42b0-b1af-0eec8687000f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad142186-b911-4d16-b253-73c0e9589db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf0c1c-412e-4bc7-bf36-16ebf138b80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "c48d82ff-da0a-4c1d-9570-52b08e031890"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
